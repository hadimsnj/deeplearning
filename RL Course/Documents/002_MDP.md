* **MDP:**  A classical formalization of sequential decision making, where actions influence not just immediate rewards, but also subsequent situations, or states, and through those future rewards.
* The need to balance immediate and delayed rewards is central to MDPs.
* In MDPs, the value of actions $q_*(s, a)$ and states $v_*(s)$ is estimated based on optimal action selections.
* The probability of transitioning to a new state \($s'$\) and receiving a reward \($r$\) given that the previous state was \($s$\) and the action taken was \($a$\):

  $$
  p(s', r \mid s, a) = \Pr\{S_t = s', R_t = r \mid S_{t-1} = s, A_{t-1} = a\}

  \\

  p(s' \mid s, a) = \sum_{r \in \mathcal{R}} p(s', r \mid s, a)
  $$
* The expected reward received when taking action \($a$\) in state \($s$\):

  $$
  r(s, a) = \mathbb{E}[R_t \mid S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathcal{R}} r \sum_{s' \in \mathcal{S}} p(s', r \mid s, a)
  \\

  r(s, a, s') = \mathbb{E}[R_t \mid S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in \mathcal{R}} r \frac{p(s', r \mid s, a)}{p(s' \mid s, a)}
  $$
* The agent's goal is to maximize the cumulative reward over time.
* **Episodic tasks** : The agent's interaction with the environment is broken down into distinct episodes, each with a clear beginning and end.
* **Continuing tasks** : Involve an ongoing interaction without a natural endpoint
* **Return($G_t$):** The total accumulated reward that an agent receives, starting from a specific time step \( t \).

$$
G_t = R_{t+1} + R_{t+2} + R_{t+3} + \dots + R_T = \sum_{k=0}^{T-t-1} R_{t+k+1}


$$

* **Discounting.:** The agent tries to select actions so that the sum of the discounted rewards it receives over the future is maximized

  $$
  G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
  $$

  * The discount factor \( $\gamma$ \) takes a value between 0 and 1, inclusive (0 $\leq$ $\gamma$ $\leq$ 1).
  * 0 makes the agent **short-sighted**
  * 1 makes the agent more **far-sighted**
* **State-Value Function ($v_\pi(s)$):** Represents the expected return (cumulative future reward) starting from a state ($s$) and following a policy ($\pi$) thereafter.

  $$
  v_\pi(s) = \mathbb{E}_\pi [G_t \mid S_t = s]
  $$


* **Action-Value Function ($q_\pi(s, a)$\):** Represents the expected return starting from a state ($s$), taking an action ($a$), and then following policy ($\pi$) thereafter.

  $$
  q_\pi(s, a) = \mathbb{E}_\pi [G_t \mid S_t = s, A_t = a]
  $$


* **Optimal Policy ($\pi^*$):** Maximizes the expected return from each state, compared to all other policies.
* **Optimal State-Value Function ($v^*(s)$\):** The maximum expected return that can be achieved starting from state \($s$\) and following an optimal policy thereafter

$$
v^*(s) = \max_\pi v_\pi(s) = \max_a q^*(s, a)
$$

* **Optimal Action-Value Function ($q^*(s, a)$\):** The maximum expected return starting from state \($s$\), taking action \($a$\), and following an optimal policy thereafter.

$$
q^*(s, a) = \max_\pi q_\pi(s, a)
$$


* **Bellman Optimality Equation:**

$$
v^*(s) = \max_a \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma v^*(s') \right]

\\

q^*(s, a) = \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma \max_{a'} q^*(s', a') \right]
$$

---


### Summary of other resource



- **Agent & Environment Interface:** At each step $t$ the agent receives a state $S_t$, performs an action $A_t$ and receives a reward $R_{t+1}$. The action is chosen according to a policy function $\pi$.
- The **total return** $G_t$ is the sum of all rewards starting from time t . **Future rewards** are discounted at a discount rate $\gamma^k$.
- **Markov property:** The environment's response at time $t+1$ depends only on the state and action representations at time $t$. The future is independent of the past given the present. Even if an environment doesn't fully satisfy the Markov property we still treat it as if it is and try to construct the state representation to be approximately Markov.
- **Markov Decision Process (MDP):** Defined by a state set $S$, action set $A$ and one-step dynamics $p(s',r | s,a)$. If we have complete knowledge of the environment we know the transition dynamic. In practice, we often don't know the full MDP (but we know that it's some MDP).
- The **Value Function** $v(s)$ estimates how "good" it is for an agent to be in a particular state. More formally, it's the expected return $G_t$ given that the agent is in state $s$. $v(s) = Ex[G_t | S_t = s]$. Note that the value function is specific to a given policy $\pi$.
- **Action Value function:** $q(s, a)$ estimates how "good" it is for an agent to be in states and take action a. Similar to the value function, but also considers the action.
- The **Bellman equation** expresses the relationship between the value of a state and the values of its successor states. It can be expressed using a "backup" diagram. Bellman equations exist for both the value function and the action value function.
- **Value functions** define an ordering over policies. A policy $p1$ is better than $p2$ if $v_{p1}(s) >= v_{p2}(s)$ for all states $s$. For MDPs, there exist one or more optimal policies that are better than or equal to all other policies.
- The **optimal state value function** $v^*(s)$ is the value function for the optimal policy. Same for $q^*(s, a)$. **The Bellman Optimality Equation defines how the optimal value of a state is related to the optimal value of successor states. It has a "max" instead of an average.**
