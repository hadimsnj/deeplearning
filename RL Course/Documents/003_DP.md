- **Dynamic programming (DP):** refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a MDP.
- DP methods can be applied to problems with continuous state and action spaces, but exact solutions are typically possible only in special cases.
- A common approach for continuous problems is to quantize the state and action spaces and then apply finite-state DP methods.
- The key idea of DP, and reinforcement learning in general, is the use of value functions to organize and structure the search for good policies.
- **The Bellman optimality equations:**

$$
v^*(s) = \max_{a} \mathbb{E} \left[ R_{t+1} + \gamma v_*(S_{t+1}) \mid S_t = s, A_t = a \right] = \max_{a} \sum_{s',r} p(s', r \mid s, a) \left[ r + \gamma v_*(s') \right]

\\

q^*(s, a) = \mathbb{E} \left[ R_{t+1} + \gamma \max_{a'} q_*(S_{t+1}, a') \mid S_t = s, A_t = a \right] = \sum_{s',r} p(s', r \mid s, a) \left[ r + \gamma \max_{a'} q_*(s', a') \right]
$$

- **Policy Evaluation**: Computes the state-value function $v_\pi$ for a given policy $\pi$. (the prediction problem in dynamic programming.)

  $$
  v_\pi(s) = \mathbb{E}_\pi \left[ R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s \right]  = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r \mid a) \left[ r + \gamma v_\pi(s') \right]
  $$
- The goal is to determine $v_\pi(s)$, the expected return from state $s$ under policy $\pi$.
- **Iterative Policy Evaluation**: Starts with an arbitrary value function. Then uses the Bellman equation for $v_\pi$ to iteratively update the value function:

  $$
  v_{k+1}(s) = \mathbb{E}_\pi \left[ R_{t+1} + \gamma v_k(S_{t+1}) \mid S_t = s \right]  = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma v_k(s') \right]
  $$
- This iterative process continues until the value function converges to $v_\pi$, which is the fixed point of the update rule. Convergence is guaranteed under certain conditions, such as a discount factor $\gamma < 1$ or guaranteed eventual termination of the policy.
- **Two-array (Jacobi-style)**: New values are computed without altering old ones.
- **In-place (Gauss-Seidel-style)**: Updates overwrite old values immediately, often converging faster.
- **Policy Improvement**: Our reason for computing the value function for a policy is to help find better policies. If the value function $v_\pi$ for a policy $\pi$ is known, the goal is to determine if a different action $a \neq \pi(s)$ would yield a better outcome in state $s$.
- The **Policy Improvement Theorem** states that if for all states $s$:

$$
q_\pi(s, \pi'(s)) \geq v_\pi(s),
$$

- The new policy $\pi'$ is at least as good as $\pi$. If there is strict inequality for any state, then $\pi'$ is strictly better.
- The theorem suggests that if it is better to take action $a$ once in state $s$ and then follow $\pi$, it is better to change the policy to always choose $a$ in $s$.
- **Greedy Policy**: A new policy $\pi'$ that selects actions to maximize the expected return in the short term, based on the current value function $v_\pi$:

$$
\pi'(s) = \arg\max_a q_\pi(s, a) = \arg\max_a \mathbb{E} \left[ R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s, A_t = a \right]
$$

- By making the policy greedy with respect to the value function, we ensure that the policy improves or at least remains unchanged, unless it is already optimal.
- If the new greedy policy $\pi'$ is not better than the original policy, both policies are optimal, satisfying the Bellman optimality equation:

$$
v_\pi(s) = \max_a \mathbb{E} \left[ R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s, A_t = a \right]
$$

- In general, the process of policy improvement involves evaluating the current policy and improving it by making it greedy with respect to its value function, iteratively moving towards optimality.

---

### Summary of other resource

- **Dynamic Programming (DP)** methods assume that we have a perfect model of the environment's Markov Decision Process (MDP). That's usually not the case in practice, but it's important to study DP anyway.
- **Policy Evaluation:** Calculates the state-value function $V(s)$ for a given policy. In DP this is done using a "full backup". At each state, we look ahead one step at each possible action and next state. We can only do this because we have a perfect model of the environment.
- Full backups are basically the Bellman equations turned into updates.
- **Policy Improvement:** Given the correct state-value function for a policy we can act greedily with respect to it (i.e. pick the best action at each state). Then we are guaranteed to improve the policy or keep it fixed if it's already optimal.
- **Policy Iteration:** Iteratively perform Policy Evaluation and Policy Improvement until we reach the optimal policy.
- **Value Iteration:** Instead of doing multiple steps of Policy Evaluation to find the "correct" V(s) we only do a single step and improve the policy immediately. In practice, this converges faster.
- **Generalized Policy Iteration:** The process of iteratively doing policy evaluation and improvement. We can pick different algorithms for each of these steps but the basic idea stays the same.
- **DP methods bootstrap:** They update estimates based on other estimates (one step ahead).
